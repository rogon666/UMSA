{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Maestría en Ciencia y Análisis de Datos\n",
        "# Universidad Mayor de San Andrés\n",
        "# ----------------------------------------------------------\n",
        "#            Machine Learning y Deep Learning\n",
        "# ----------------------------------------------------------\n",
        "#         Rolando Gonzales Martinez, Agosto 2024\n",
        "# ==========================================================\n",
        "#        Redes neuronales (perceptron) multicapa\n",
        "# ==========================================================\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Función de activación Sigmoide\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Derivada de la función Sigmoide\n",
        "def sigmoid_derivative(z):\n",
        "    return z * (1 - z)\n",
        "\n",
        "# Función de pérdida de error cuadrático medio\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# MLP simple con una capa oculta\n",
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Inicialización de pesos aleatorios\n",
        "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
        "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
        "        self.bias_hidden = np.random.randn(hidden_size)\n",
        "        self.bias_output = np.random.randn(output_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Paso 1: Propagación hacia adelante\n",
        "        self.z1 = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.a1 = sigmoid(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.weights_hidden_output) + self.bias_output\n",
        "        self.a2 = sigmoid(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def backward(self, X, y, output, learning_rate):\n",
        "        # Paso 2: Retropropagación\n",
        "        # Error en la capa de salida\n",
        "        output_error = y - output\n",
        "        output_delta = output_error * sigmoid_derivative(output)\n",
        "\n",
        "        # Error en la capa oculta\n",
        "        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n",
        "        hidden_delta = hidden_error * sigmoid_derivative(self.a1)\n",
        "\n",
        "        # Actualización de pesos y sesgos\n",
        "        self.weights_hidden_output += np.dot(self.a1.T, output_delta) * learning_rate\n",
        "        self.bias_output += np.sum(output_delta, axis=0) * learning_rate\n",
        "        self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate\n",
        "        self.bias_hidden += np.sum(hidden_delta, axis=0) * learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        loss_history = []\n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, output, learning_rate)\n",
        "            loss = mse_loss(y, output)\n",
        "            loss_history.append(loss)\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "        return loss_history\n",
        "\n",
        "# Datos de entrenamiento (XOR problem)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Crear una red neuronal con 2 entradas, 2 neuronas en la capa oculta, y 1 salida\n",
        "mlp = MLP(input_size=2, hidden_size=2, output_size=1)\n",
        "\n",
        "# Entrenar la red\n",
        "epochs = 10000\n",
        "learning_rate = 0.1\n",
        "loss_history = mlp.train(X, y, epochs=epochs, learning_rate=learning_rate)\n",
        "\n",
        "# Probar la red después del entrenamiento\n",
        "output = mlp.forward(X)\n",
        "print(\"Predicciones después del entrenamiento:\")\n",
        "print(output)\n",
        "\n",
        "# Graficar la pérdida durante el entrenamiento\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(epochs), loss_history, label='Loss')\n",
        "plt.xlabel('Epocas (Epochs)')\n",
        "plt.ylabel('Perdida (Loss, L)')\n",
        "plt.title('Evolución de la Pérdida Durante el Entrenamiento')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Graficar las predicciones finales y la línea de separación aprendida\n",
        "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
        "                     np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "Z = mlp.forward(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.contourf(xx, yy, Z, levels=[0, 0.5, 1], alpha=0.5, cmap=plt.cm.bwr)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=100, edgecolors='k', marker='o', cmap=plt.cm.bwr)\n",
        "plt.xlabel('Entrada 1')\n",
        "plt.ylabel('Entrada 2')\n",
        "plt.title('Espacio de Decisión Aprendido por el MLP para el problema XOR')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JulZ5ZzK1s0Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}